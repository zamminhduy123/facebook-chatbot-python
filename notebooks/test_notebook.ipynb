{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f6bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from google.genai import errors\n",
    "from deepeval import assert_test\n",
    "from deepeval.metrics import GEval, RoleAdherenceMetric\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gemini_prompt import MODEL_ID, get_chat_config, get_evaluator_config\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\"\"\"\n",
    "Conversational Completeness\n",
    "Conversational Relevancy\n",
    "Conversational G-Eval\n",
    "Knowledge Retention\n",
    "Role Adherence\n",
    "Bias\n",
    "Toxicity\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "with open(\"gemini_api_keys.txt\", \"r\") as fhandle:\n",
    "    API_KEYS = fhandle.read()\n",
    "    API_KEYS = API_KEYS.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8457a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished initializing with 4/4 client(s).\n",
      "Finished initializing with 4/4 client(s).\n"
     ]
    }
   ],
   "source": [
    "class GeminiLLM(DeepEvalBaseLLM):\n",
    "    \"\"\"Class to implement Vertex AI for DeepEval\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str,\n",
    "        model_name: str,\n",
    "        config: GenerateContentConfig = None,\n",
    "    ):\n",
    "\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.client\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        client = self.load_model()\n",
    "\n",
    "        res = client.models.generate_content(\n",
    "            model=self.model_name,\n",
    "            contents=prompt,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        return res.text\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        client = self.load_model()\n",
    "\n",
    "        res = await client.aio.models.generate_content(\n",
    "            model=self.model_name,\n",
    "            contents=prompt,\n",
    "            config=self.config,\n",
    "        )\n",
    "\n",
    "        return res.text\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "\n",
    "class GeminiLLMManager(GeminiLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_keys: List[str],\n",
    "        model_name: str,\n",
    "        config: GenerateContentConfig = None,\n",
    "    ):\n",
    "\n",
    "        self.clients = self.init_clients(api_keys)\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "\n",
    "    def init_clients(self, api_keys) -> List:\n",
    "        clients = []\n",
    "        for api_key in api_keys:\n",
    "            client = genai.Client(api_key=api_key)\n",
    "            try:\n",
    "\n",
    "                client.models.get(model=MODEL_ID)\n",
    "                clients.append(\n",
    "                    {\n",
    "                        \"client\": client,\n",
    "                        \"time\": time.time(),\n",
    "                        \"rpm\": 15,\n",
    "                        \"rate\": 0,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid API key ...{api_key[-4:]}. Reason: {e}.\\nSkipping...\")\n",
    "\n",
    "        print(f\"Finished initializing with {len(clients)}/{len(api_keys)} client(s).\")\n",
    "\n",
    "        return clients\n",
    "\n",
    "    def load_model(self):\n",
    "        random.shuffle(self.clients)\n",
    "        while True:\n",
    "            min_wait_time = 60\n",
    "            now = time.time()\n",
    "            for client in self.clients:\n",
    "                time_since_last_minute = now - client[\"time\"]\n",
    "                wait_time = 60 - time_since_last_minute\n",
    "                min_wait_time = min(min_wait_time, wait_time)\n",
    "                if time_since_last_minute > 60:\n",
    "                    client[\"time\"] = now\n",
    "                    client[\"rate\"] = 0\n",
    "                    return client\n",
    "\n",
    "                if time_since_last_minute <= 60 and client[\"rate\"] < client[\"rpm\"]:\n",
    "                    return client\n",
    "\n",
    "            print(f\"Out of RPM, waiting {min_wait_time:.2f}s\")\n",
    "            for _ in tqdm(range(int(min_wait_time + 1))):\n",
    "                time.sleep(1)\n",
    "\n",
    "    async def a_load_model(self):\n",
    "        random.shuffle(self.clients)\n",
    "        min_wait_time = 60\n",
    "        while True:\n",
    "            now = time.time()\n",
    "            for client in self.clients:\n",
    "                time_since_last_minute = now - client[\"time\"]\n",
    "                wait_time = 60 - time_since_last_minute\n",
    "                min_wait_time = min(min_wait_time, wait_time)\n",
    "                if time_since_last_minute > 60:\n",
    "                    client[\"time\"] = now\n",
    "                    client[\"rate\"] = 0\n",
    "                    return client\n",
    "\n",
    "                if time_since_last_minute <= 60 and client[\"rate\"] < client[\"rpm\"]:\n",
    "                    return client\n",
    "\n",
    "            print(f\"Out of RPM, waiting {min_wait_time:.2f}s\")\n",
    "            await asyncio.sleep(min_wait_time)\n",
    "\n",
    "    # def generate(self, prompt: str) -> str:\n",
    "    #     client = self.load_model()\n",
    "    #     res = client[\"client\"].models.generate_content(\n",
    "    #         model=self.model_name,\n",
    "    #         contents=prompt,\n",
    "    #         config=self.config,\n",
    "    #     )\n",
    "    #     client[\"rate\"] += 1\n",
    "    #     return res.text\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        retry_count = 100\n",
    "        retry_delay_max = 60\n",
    "        fibo_delays = [1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n",
    "\n",
    "        e = None\n",
    "        for t in range(retry_count):\n",
    "            try:\n",
    "                client = self.load_model()\n",
    "                res = client[\"client\"].models.generate_content(\n",
    "                    model=self.model_name,\n",
    "                    contents=prompt,\n",
    "                    config=self.config,\n",
    "                )\n",
    "                client[\"rate\"] += 1\n",
    "                return res.text\n",
    "\n",
    "            except Exception as e:\n",
    "                if isinstance(e, errors.ClientError):\n",
    "                    if e.code == 429:\n",
    "                        client[\"rate\"] = 15\n",
    "\n",
    "                client[\"rate\"] += 1\n",
    "                print(f\"Failed to call gemini API. Reason: {e}.\")\n",
    "                print(f\"Retry {t+1}/{retry_count}...\")\n",
    "\n",
    "                retry_idx = min(len(fibo_delays) - 1, t)\n",
    "                retry_delay = min(fibo_delays[retry_idx], retry_delay_max)\n",
    "\n",
    "                time.sleep(retry_delay)\n",
    "\n",
    "        print(f\"Failed to generate. Skipping...\")\n",
    "\n",
    "        raise TimeoutError(\n",
    "            f\"Failed to generate content after {retry_count} retries. Last exception: {e}\"\n",
    "        )\n",
    "\n",
    "    # async def a_generate(self, prompt: str) -> str:\n",
    "    #     client = self.load_model()\n",
    "    #     res = await client[\"client\"].aio.models.generate_content(\n",
    "    #         model=self.model_name,\n",
    "    #         contents=prompt,\n",
    "    #         config=self.config,\n",
    "    #     )\n",
    "    #     client[\"rate\"] += 1\n",
    "    #     return res.text\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        retry_count = 100\n",
    "        retry_delay = 0.2\n",
    "\n",
    "        e = None\n",
    "        for t in range(retry_count):\n",
    "            try:\n",
    "                client = await self.a_load_model()\n",
    "                res = await client[\"client\"].aio.models.generate_content(\n",
    "                    model=self.model_name,\n",
    "                    contents=prompt,\n",
    "                    config=self.config,\n",
    "                )\n",
    "                client[\"rate\"] += 1\n",
    "                return res.text\n",
    "\n",
    "            except Exception as ex:\n",
    "                e = ex\n",
    "                print(f\"Failed to call gemini API. Reason: {e}.\")\n",
    "                print(f\"Retry {t+1}/{retry_count}...\")\n",
    "                await asyncio.sleep(retry_delay)\n",
    "\n",
    "        print(f\"Failed to generate. Skipping...\")\n",
    "\n",
    "        raise TimeoutError(\n",
    "            f\"Failed to generate content after {retry_count} retries. Last exception: {e}\"\n",
    "        )\n",
    "\n",
    "\n",
    "eval_llm = GeminiLLM(\n",
    "    api_key=API_KEY,\n",
    "    model_name=MODEL_ID,\n",
    "    config=get_evaluator_config(),\n",
    ")\n",
    "\n",
    "\n",
    "gemini_llm = GeminiLLM(\n",
    "    api_key=API_KEY,\n",
    "    model_name=MODEL_ID,\n",
    "    config=get_chat_config(),\n",
    ")\n",
    "\n",
    "\n",
    "eval_llm_manager = GeminiLLMManager(\n",
    "    api_keys=API_KEYS,\n",
    "    model_name=MODEL_ID,\n",
    "    config=get_evaluator_config(),\n",
    ")\n",
    "\n",
    "\n",
    "gemini_llm_manager = GeminiLLMManager(\n",
    "    api_keys=API_KEYS,\n",
    "    model_name=MODEL_ID,\n",
    "    config=get_chat_config(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ad1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metric(test_info: Dict, eval_llm=None):\n",
    "    return GEval(\n",
    "        name=test_info[\"title\"],\n",
    "        criteria=test_info[\"criteria\"],\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "        model=eval_llm,\n",
    "        async_mode=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_test_cases_info(\n",
    "    json_filepath: str,\n",
    "    llm: GeminiLLM = None,\n",
    "    retry: float = 3,\n",
    "    retry_wait: float = 0.5,\n",
    "    wait_time: int = 60,\n",
    "    rate_limit: int = 15,\n",
    "    overwrite: bool = False,\n",
    "):\n",
    "    with open(json_filepath, \"r\", encoding=\"utf8\") as fhandle:\n",
    "        contents = json.load(fhandle)\n",
    "\n",
    "    request_count = 0\n",
    "    for i, item in enumerate(contents):\n",
    "        item: Dict\n",
    "        if isinstance(item[\"criteria\"], list):\n",
    "            item[\"criteria\"] = \" \".join(item[\"criteria\"])\n",
    "\n",
    "        if llm:\n",
    "            for t in range(retry):\n",
    "\n",
    "                if request_count >= rate_limit:\n",
    "                    request_count = 0\n",
    "                    print(f\"rate limit hit, waiting {wait_time}s\")\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "                actual_output = item.get(\"actual_output\")\n",
    "                if actual_output != None and not overwrite:\n",
    "                    print(f\"{i} already exists, continue\")\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    output = llm.generate(item[\"question\"])\n",
    "                    item[\"actual_output\"] = output\n",
    "                    request_count += 1\n",
    "                    print(f\"{i} Successfully generated output.\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    item[\"actual_output\"] = None\n",
    "                    print(f\"Failed to generate output for index ({i}). Reason: {e}\")\n",
    "                    print(f\"Retry {t+1}/{retry} ...\")\n",
    "                time.sleep(retry_wait)\n",
    "    print(\"Done!!!\")\n",
    "\n",
    "    return contents\n",
    "\n",
    "def load_test_case_infos(json_path:str):\n",
    "    with open(json_path, \"r\", encoding=\"utf8\") as fhandle:\n",
    "        content = json.load(fhandle)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_infos = build_test_cases_info(\"test1.json\", gemini_llm)\n",
    "with open(\"test_suite_1.json\", \"w\", encoding=\"utf8\") as fhandle:\n",
    "    json.dump(test_infos, fhandle, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "501c260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Successfully generated output.\n",
      "1 Successfully generated output.\n",
      "2 Successfully generated output.\n",
      "3 Successfully generated output.\n",
      "4 Successfully generated output.\n",
      "5 Successfully generated output.\n",
      "6 Successfully generated output.\n",
      "7 Successfully generated output.\n",
      "8 Successfully generated output.\n",
      "9 Successfully generated output.\n",
      "10 Successfully generated output.\n",
      "11 Successfully generated output.\n",
      "12 Successfully generated output.\n",
      "13 Successfully generated output.\n",
      "14 Successfully generated output.\n",
      "rate limit hit, waiting 60s\n",
      "15 Successfully generated output.\n",
      "16 Successfully generated output.\n",
      "17 Successfully generated output.\n",
      "18 Successfully generated output.\n",
      "19 Successfully generated output.\n",
      "20 Successfully generated output.\n",
      "21 Successfully generated output.\n",
      "22 Successfully generated output.\n",
      "23 Successfully generated output.\n",
      "24 Successfully generated output.\n",
      "25 Successfully generated output.\n",
      "26 Successfully generated output.\n",
      "27 Successfully generated output.\n",
      "28 Successfully generated output.\n",
      "29 Successfully generated output.\n",
      "rate limit hit, waiting 60s\n",
      "30 Successfully generated output.\n",
      "31 Successfully generated output.\n",
      "32 Successfully generated output.\n",
      "33 Successfully generated output.\n",
      "34 Successfully generated output.\n",
      "35 Successfully generated output.\n",
      "36 Successfully generated output.\n",
      "37 Successfully generated output.\n",
      "38 Successfully generated output.\n",
      "39 Successfully generated output.\n",
      "40 Successfully generated output.\n",
      "41 Successfully generated output.\n",
      "42 Successfully generated output.\n",
      "43 Successfully generated output.\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "test_infos = build_test_cases_info(\"test2.json\", gemini_llm)\n",
    "with open(\"test_suite_2.json\", \"w\", encoding=\"utf8\") as fhandle:\n",
    "    json.dump(test_infos, fhandle, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebddef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_infos = build_test_cases_info(\"test3.json\", gemini_llm)\n",
    "with open(\"test_suite_3.json\", \"w\", encoding=\"utf8\") as fhandle:\n",
    "    json.dump(test_infos, fhandle, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c117dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepeval_test_case(\n",
    "    test_info: Dict,\n",
    "    eval_llm: GeminiLLM,\n",
    "    llm: GeminiLLM = None,\n",
    ") -> Dict:\n",
    "\n",
    "    actual_output = test_info[\"actual_output\"]\n",
    "    if llm:\n",
    "        actual_output = llm.generate(test_info[\"question\"])\n",
    "\n",
    "    test_case = LLMTestCase(input=test_info[\"question\"], actual_output=actual_output)\n",
    "    metric = parse_metric(test_info, eval_llm)\n",
    "\n",
    "    result = {\n",
    "        \"title\": test_info[\"title\"],\n",
    "        \"input\": test_case.input,\n",
    "        \"output\": test_case.actual_output,\n",
    "        \"criteria\": metric.criteria,\n",
    "        \"score\": None,\n",
    "        \"reason\": None,\n",
    "        \"evaluation_steps\": None,\n",
    "        \"evaluation_model\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        score = metric.measure(test_case,False)\n",
    "        result[\"score\"] = score\n",
    "        result[\"reason\"] = metric.reason\n",
    "        result[\"evaluation_steps\"] = metric.evaluation_steps\n",
    "        result[\"evaluation_model\"] = metric.evaluation_model\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to eval test case: {test_info['title']}\")\n",
    "        print(e)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def deepeval_test_cases(\n",
    "    test_infos: List[Dict],\n",
    "    eval_llm: GeminiLLM,\n",
    "    llm: GeminiLLM = None,\n",
    "):\n",
    "    for test_info in test_infos:\n",
    "        result = deepeval_test_case(test_info, eval_llm)\n",
    "        yield result\n",
    "\n",
    "\n",
    "def evaluate_tests(\n",
    "    output_json_path: str,\n",
    "    test_infos: List[Dict],\n",
    "    eval_llm: GeminiLLM,\n",
    "    overwrite: bool = False,\n",
    "):\n",
    "    # check if output_json_path already exist:\n",
    "    # _test_infos = []\n",
    "    # if os.path.exists(output_json_path) and not overwrite:\n",
    "    #     with open(output_json_path, \"r\", encoding=\"utf8\") as fhandle:\n",
    "    #         outputs = json.load(fhandle)\n",
    "    #     for output in outputs:\n",
    "    #         if output.get(\"score\") == None:\n",
    "    #             _test_infos.append({\n",
    "    #                 \"title\": output[\"title\"],\n",
    "    #                 \"question\": output[\"input\"],\n",
    "    #                 \"criteria\": output[\"criteria\"],\n",
    "    #                 \"actual_output\": output[\"output\"],\n",
    "    #             })\n",
    "    # else:\n",
    "    #     _test_infos = test_infos\n",
    "\n",
    "    test_result_generator = deepeval_test_cases(test_infos, eval_llm)\n",
    "    with open(output_json_path, \"w\", encoding=\"utf8\") as fhandle:\n",
    "        fhandle.write(\"[\\n\")\n",
    "\n",
    "    first = True\n",
    "    for result in test_result_generator:\n",
    "        with open(output_json_path, \"a\", encoding=\"utf8\") as fhandle:\n",
    "            if not first:\n",
    "                fhandle.write(\",\\n\")\n",
    "            json.dump(result, fhandle, ensure_ascii=False, indent=4)\n",
    "            first = False\n",
    "\n",
    "    with open(output_json_path, \"a\", encoding=\"utf8\") as fhandle:\n",
    "        fhandle.write(\"\\n]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c3be544",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_infos = load_test_case_infos(\"test_suite_2.json\")\n",
    "evaluate_tests(\"test_output_2.json\", test_infos, eval_llm_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5f37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62f53047",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_paths = [\n",
    "    \"test_output_1.json\",\n",
    "    \"test_output_2.json\",\n",
    "    \"test_output_3.json\",\n",
    "]\n",
    "\n",
    "for test_output_path in test_output_paths:\n",
    "    df = pd.read_json(test_output_path)\n",
    "    out_path, _= os.path.splitext(test_output_path)\n",
    "    out_path = f\"{out_path}.csv\"\n",
    "    df.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bfaaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
